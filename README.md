# Basic Kafka Hands-On

A hands-on project demonstrating Apache Kafka fundamentals with Python, featuring a producer-consumer architecture for processing order events.

## About The Project

This project provides a practical introduction to Apache Kafka by implementing a complete event-driven system for order processing. It showcases:

- **Event Production**: Publishing order events to Kafka topics with message batching
- **Event Consumption**: Subscribing to topics and processing messages in real-time
- **Data Validation**: Using Pydantic models for type-safe message serialization/deserialization
- **Modern Kafka**: Running Kafka in KRaft mode (without ZooKeeper)
- **Clean Architecture**: Modular design with separate models, Kafka utilities, and application logic
- **Comprehensive Testing**: Unit tests for producers, consumers, and data models

### Project Structure

```
basic-kafka-hands-on/
â”œâ”€â”€ kafka/              # Kafka producer and consumer wrappers
â”‚   â”œâ”€â”€ producer.py
â”‚   â””â”€â”€ consumer.py
â”œâ”€â”€ models/             # Pydantic data models
â”‚   â””â”€â”€ order.py
â”œâ”€â”€ tests/              # Unit tests
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_kafka_producer.py
â”‚   â”œâ”€â”€ test_kafka_consumer.py
â”‚   â””â”€â”€ test_order.py
â”œâ”€â”€ order_producer.py   # Order producer application
â”œâ”€â”€ order_consumer.py   # Order consumer application
â”œâ”€â”€ docker-compose.yml  # Kafka infrastructure
â””â”€â”€ requirements.txt    # Python dependencies
```

### Built With

* [![Python][Python-badge]][Python-url]
* [![Kafka][Kafka-badge]][Kafka-url]
* [![Pydantic][Pydantic-badge]][Pydantic-url]
* [![Docker][Docker-badge]][Docker-url]
* [![Pytest][Pytest-badge]][Pytest-url]

**Key Technologies:**
- **Python 3.11+** - Core programming language
- **Apache Kafka 7.8.6** - Distributed event streaming platform (KRaft mode)
- **Pydantic 2.10.5** - Data validation and settings management
- **confluent-kafka 2.12.2** - High-performance Kafka client library
- **pytest 7.4.3** - Testing framework
- **Docker** - Containerized Kafka deployment

## Getting Started

Follow these instructions to get the project up and running on your local machine.

### Prerequisites

* **Python 3.11 or higher**
  ```bash
  python --version
  ```

* **Docker and Docker Compose**
  ```bash
  docker --version
  docker compose version
  ```

* **pip** (Python package installer)
  ```bash
  pip --version
  ```

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/sar-joshi/basic-kafka-hands-on.git
   cd basic-kafka-hands-on
   ```

2. **Create a virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install Python dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Start Kafka with Docker Compose**
   ```bash
   docker compose up -d
   ```

5. **Verify Kafka is running**
   ```bash
   docker ps
   ```
   You should see the `kafka` container running on port 9092.

6. **Create the `orders` topic** (optional - auto-created on first message)
   ```bash
   docker exec -it kafka kafka-topics --create \
     --bootstrap-server localhost:9092 \
     --topic orders \
     --partitions 3 \
     --replication-factor 1
   ```

## Usage

### Running the Producer

The producer creates and sends order events to the `orders` topic:

```bash
python order_producer.py
```

**Example Output:**
```
âœ… Delivered: {"order_id":"abc-123","customer_name":"John Doe","item":"MacBook Pro",...}
âœ… Topic: orders, Partition: 0, Offset: 42
```

### Running the Consumer

The consumer subscribes to the `orders` topic and processes incoming messages:

```bash
python order_consumer.py
```

**Example Output:**
```
ğŸ”µ Consumer started. Waiting for messages...
Press Ctrl+C to stop.

âœ… Order received: abc-123
   Customer: John Doe, Item: MacBook Pro, Qty: 1
```

Press `Ctrl+C` to stop the consumer gracefully.

### Running Tests

Execute the test suite to verify functionality:

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_kafka_producer.py -v

# Run with coverage
pytest tests/ --cov=kafka --cov=models -v
```

### Useful Kafka Commands

**List all topics:**
```bash
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092
```

**Describe a topic:**
```bash
docker exec -it kafka kafka-topics --describe --topic orders --bootstrap-server localhost:9092
```

**Consume messages from CLI:**
```bash
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic orders \
  --from-beginning
```

**Check consumer group status:**
```bash
docker exec -it kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --describe \
  --group order-consumer
```

### Stopping the Project

1. **Stop the consumer/producer** - Press `Ctrl+C`

2. **Stop Kafka**
   ```bash
   docker compose down
   ```

3. **Remove volumes (clean slate)**
   ```bash
   docker compose down -v
   ```

## Key Concepts Demonstrated

### Producer Concepts
- âœ… Message batching and buffering
- âœ… Delivery callbacks and error handling
- âœ… Topic partitioning
- âœ… JSON serialization with Pydantic

### Consumer Concepts
- âœ… Consumer groups
- âœ… Offset management (earliest/latest)
- âœ… Polling and message processing
- âœ… Graceful shutdown
- âœ… Error handling for malformed messages

### Kafka Architecture
- âœ… KRaft mode (no ZooKeeper required)
- âœ… Topics and partitions
- âœ… Producers and consumers
- âœ… Message ordering within partitions

## Understanding Kafka Partitioning

Partitioning is a fundamental concept in Kafka that enables scalability and ordering guarantees. Let's dive deep into how it works.

### What is Topic Partitioning?

A Kafka **topic** is divided into multiple **partitions**. Think of partitions as separate "lanes" within a topic that allow parallel processing.

**Visual Structure:**
```
Topic: "orders"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                 â”‚
â”‚  Partition 0: [msg1] [msg2] [msg3] [msg4] ...  â”‚
â”‚                                                 â”‚
â”‚  Partition 1: [msg5] [msg6] [msg7] [msg8] ...  â”‚
â”‚                                                 â”‚
â”‚  Partition 2: [msg9] [msg10] [msg11] ...       â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Use Partitions?

#### 1. Parallelism and Scalability
```
Producer 1 â”€â”€â”
             â”œâ”€â”€â†’ Partition 0 â”€â”€â†’ Consumer A
Producer 2 â”€â”€â”¤
             â”œâ”€â”€â†’ Partition 1 â”€â”€â†’ Consumer B
Producer 3 â”€â”€â”¤
             â””â”€â”€â†’ Partition 2 â”€â”€â†’ Consumer C
```
- Multiple consumers can read from different partitions simultaneously
- Increases throughput (more messages processed per second)
- Horizontal scaling by adding more partitions and consumers

#### 2. Load Distribution
- Messages are distributed across partitions
- Prevents any single partition from becoming a bottleneck
- Better resource utilization across the cluster

#### 3. Fault Tolerance
- Each partition can have replicas on different brokers
- If one broker fails, data remains available from replicas
- Ensures high availability and durability

### How Messages are Assigned to Partitions

Kafka uses three strategies to determine which partition a message goes to:

#### Strategy 1: Round-Robin (No Key)
```python
# Messages distributed evenly across partitions
producer.produce(topic="orders", value=message)
# Distribution: P0 â†’ P1 â†’ P2 â†’ P0 â†’ P1 â†’ P2...
```
- **Use case:** When order doesn't matter
- **Advantage:** Even distribution across partitions

#### Strategy 2: Key-Based Partitioning (Recommended)
```python
# All messages with the same key go to the same partition
producer.produce(
    topic="orders",
    key="customer-123",  # Same key = Same partition
    value=message
)
```
- **Use case:** When you need ordering for related messages
- **Advantage:** Guarantees all messages with the same key are processed in order

#### Strategy 3: Explicit Partition Assignment
```python
# Directly specify which partition to use
producer.produce(topic="orders", partition=1, value=message)
```
- **Use case:** Custom partitioning logic
- **Advantage:** Full control over message placement

### Message Ordering Guarantees

**Critical Rule:** Kafka guarantees message order **within a partition**, but **NOT across partitions**.

#### Within a Single Partition âœ…
```
Partition 0: [Order A: created] â†’ [Order A: paid] â†’ [Order A: shipped]
             â†‘ Messages are always read in the order they were written
```

#### Across Multiple Partitions âŒ
```
Partition 0: [Order A: created] [Order A: paid]
Partition 1: [Order B: created] [Order B: paid]
Partition 2: [Order C: created] [Order C: paid]

Consumer might read in any order across partitions:
[Order B: created] [Order A: created] [Order C: paid] [Order A: paid] ...
```

### Real-World Example: Order Processing

#### âŒ Problem: Without Keys (No Ordering)

```python
# Producer sends order status updates
order1 = Order(customer_id="Alice", item="iPhone", status="created")
order2 = Order(customer_id="Alice", item="iPhone", status="paid")
order3 = Order(customer_id="Alice", item="iPhone", status="shipped")

# Without keys - goes to random partitions
producer.produce(topic="orders", value=order1.model_dump_json().encode())
producer.produce(topic="orders", value=order2.model_dump_json().encode())
producer.produce(topic="orders", value=order3.model_dump_json().encode())
```

**Result:**
```
Partition 0: [Alice: shipped]    â† Wrong! Shipped before payment!
Partition 1: [Alice: created]
Partition 2: [Alice: paid]
```

#### âœ… Solution: With Keys (Ordering Guaranteed)

```python
# Use customer_id as key - ensures all of Alice's orders go to same partition
producer.produce(
    topic="orders",
    key=order.customer_id.encode("utf-8"),  # Key ensures ordering
    value=order1.model_dump_json().encode()
)
producer.produce(
    topic="orders",
    key=order.customer_id.encode("utf-8"),
    value=order2.model_dump_json().encode()
)
producer.produce(
    topic="orders",
    key=order.customer_id.encode("utf-8"),
    value=order3.model_dump_json().encode()
)
```

**Result:**
```
Partition 1: [Alice: created] â†’ [Alice: paid] â†’ [Alice: shipped] âœ…
             â†‘ Correct order maintained!
```

### Implementing Key-Based Partitioning

Update your producer to support message keys:

**1. Update the producer wrapper:**
```python
# kafka/producer.py
def produce(self, topic: str, key: bytes = None, value: bytes, callback=None):
    """
    Produce a message to a Kafka topic.
    
    Args:
        topic: The Kafka topic to send the message to.
        key: Optional message key for partitioning.
        value: The message value as bytes.
        callback: Optional callback function for delivery reports.
    """
    self.producer.produce(topic=topic, key=key, value=value, callback=callback)
```

**2. Use keys in your application:**
```python
# order_producer.py
order = Order(
    customer_name="John Doe",
    item="MacBook Pro",
    quantity=1,
    price=100,
)

producer.produce(
    topic="orders",
    key=order.customer_id.encode("utf-8"),  # Ensures ordering per customer
    value=order.model_dump_json().encode("utf-8"),
    callback=delivery_report,
)
```

### Best Practices

| Scenario | Recommended Approach | Reason |
|----------|---------------------|---------|
| Order status updates | Use customer_id as key | Maintains order lifecycle sequence |
| User activity logs | Use user_id as key | Keeps user actions in order |
| IoT sensor data | Use device_id as key | Preserves time-series data order |
| General events (no ordering needed) | No key (round-robin) | Best load distribution |
| High-cardinality keys | Ensure even distribution | Prevents partition hotspots |

### Partitioning Quick Reference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Key Concept: Same Key â†’ Same Partition â†’ Ordered      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Producer sends messages with keys:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Customer A:  â”‚  Key: "A"  â”€â”€â”
â”‚  - Order 1   â”‚              â”‚
â”‚  - Order 2   â”‚              â”œâ”€â”€â†’ Partition 2: [A1][A2][A3] âœ…
â”‚  - Order 3   â”‚              â”‚    (All A's orders in order)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚ Customer B:  â”‚  Key: "B"  â”€â”€â”¼â”€â”€â†’ Partition 0: [B1][B2] âœ…
â”‚  - Order 1   â”‚              â”‚    (All B's orders in order)
â”‚  - Order 2   â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚ Customer C:  â”‚  Key: "C"  â”€â”€â”˜
â”‚  - Order 1   â”‚         â””â”€â”€â”€â”€â”€â”€â†’ Partition 1: [C1] âœ…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  (C's orders in order)

Consumer Group reads:
  Consumer 1 â† Partition 0 (B's orders)
  Consumer 2 â† Partition 1 (C's orders)
  Consumer 3 â† Partition 2 (A's orders)

Result: Each customer's orders processed in correct order! ğŸ¯
```

### Monitoring Partition Distribution

Check how messages are distributed across partitions:

```bash
# Describe topic to see partition details
docker exec -it kafka kafka-topics --describe --topic orders --bootstrap-server localhost:9092

# Check consumer lag per partition
docker exec -it kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --describe \
  --group order-consumer
```

**Example Output:**
```
TOPIC    PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG
orders   0          150             150             0
orders   1          148             148             0
orders   2          152             152             0
```
This shows relatively even distribution across partitions.

## Acknowledgments

* [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
* [Confluent Kafka Python](https://docs.confluent.io/kafka-clients/python/current/overview.html)
* [Pydantic Documentation](https://docs.pydantic.dev/)
* [Kafka: The Definitive Guide](https://www.confluent.io/resources/kafka-the-definitive-guide/)
* [Kafka Crash Course by TechWorld with Nana](https://youtu.be/B7CwU_tNYIE?si=dhcSRShtdqgAIQci)

---

<!-- Badge URLs -->
[Python-badge]: https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white
[Python-url]: https://www.python.org/
[Kafka-badge]: https://img.shields.io/badge/Apache%20Kafka-231F20?style=for-the-badge&logo=apache-kafka&logoColor=white
[Kafka-url]: https://kafka.apache.org/
[Pydantic-badge]: https://img.shields.io/badge/Pydantic-E92063?style=for-the-badge&logo=pydantic&logoColor=white
[Pydantic-url]: https://docs.pydantic.dev/
[Docker-badge]: https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white
[Docker-url]: https://www.docker.com/
[Pytest-badge]: https://img.shields.io/badge/Pytest-0A9EDC?style=for-the-badge&logo=pytest&logoColor=white
[Pytest-url]: https://pytest.org/
